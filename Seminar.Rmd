---
title: "Genome Wide Association Study Seminar"
author: "Riley Ashton"
date: '2018-05-16'
output: ioslides_presentation
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tibble)
library(ggplot2); theme_set(theme_minimal())
library(dplyr)
library(knitr)
library(purrr)
```
## Genome Wide Association Studies
How are SNPs and cofounding factors related to traits like diseases, blood pressure, etc.

## Regression Analysis
- A set of statistical processes for estimating relationships amoung variables
- Example: how income is related to race, education, geographic location, etc
- Example: how unemployment in related to economic growth (Okun's law) 
- Example: how force needed to compress a spring is related to distance (Hooke's law)


## Simple Linear Regression Intuition
Use a line to determine the relationship between a single covariate/predictor
and a single response. Out of all possible lines, the best choice is the line 
that minimizes the distance between observations and itself.

## Simple Linear Regression Example
```{r simple_regression}
inputPanel(
  sliderInput("sd_slider", label = "Standard Deviation",
              min = 0, max = 10, value = 0, step = 0.5
  ),
  sliderInput("intercept_slider", label = "Fitted Line Intercept",
              min = -5, max = 5, value = 0, step = 0.5
  ),
  sliderInput("slope_slider", label = "Fitted Line Slope",
              min = -5, max = 5, value = 0, step = 0.5
  )
)
renderPlot({
  n <- 5
  tibble(covariate = rep(seq_len(n), 2), 
         response = c(seq_len(n) + rnorm(n, sd = input$sd_slider), 
                      seq_len(n) * input$slope_slider + input$intercept_slider),
         line_group = as.factor(rep(seq_len(n), 2)),
         Group = as.factor(c(rep("Observed",5), rep("Fitted",5)))
         ) %>%
  ggplot(., aes(x = covariate, y = response, group = line_group, colour = Group)) +
    geom_line(colour = "grey30") +
    geom_abline(slope = input$slope_slider, intercept = input$intercept_slider) +
    geom_point(size = 5)
})
```


## Common Notions of Distance
Let $y_i$ denote the observed value at observation $i$

Let $\hat{y_i}$ denote the fitted value at observation $i$

Then our notion of distance can be defined by:

- Squared Error $\sum_{i = 1}^n (y_i - \hat{y_i})^2$
- Absolute Error $\sum_{i = 1}^n |y_i - \hat{y_i}|$
- Or anything else we choose!

Absolute error is more in line with our notion of error,
but squared error has 
[nicer mathematical properties](https://www.benkuhn.net/squared),
including the ability to solve its minimization in closed form.


##
```{r}
linear_model <- function(betas, data) {
  betas[1] + data$x * betas[2]
}

l_norm <- function(betas, data, degree, trim) {
  diff <- abs(data$y - linear_model(betas, data))
  (mean(diff ^ degree, trim = trim)) ^ (1/degree)
}

sim_data <- tibble(x = 1:100, y = 1:100 + rt(100, 1))
```


```{r}
inputPanel(
  numericInput("blue_degree", label = "Blue Line Degree", value = 1),
  
  sliderInput("blue_mean_trim", label = "Blue Line Mean Trim",
               min = 0, max = 0.5, value = 0, step = 0.01),
  
  numericInput("orange_degree", label = "Orange Line Degree", value = 2),
  
  sliderInput("orange_mean_trim", label = "Orange Line Mean Trim",
               min = 0, max = 0.5, value = 0, step = 0.01)
)
renderPlot({
  blue_lnorm <- partial(l_norm, data = sim_data, 
                        degree = input$blue_degree,
                        trim = input$blue_mean_trim)
  orange_lnorm <- partial(l_norm, data = sim_data,
                        degree = input$orange_degree,
                        trim = input$orange_mean_trim)
  blue_betas <- optim(c(0,0), blue_lnorm)$"par"
  orange_betas <- optim(c(0,0), orange_lnorm)$"par"
  
  ggplot(data = sim_data, aes(x = x, y = y)) +
    geom_point() +
    geom_abline(slope = blue_betas[2], intercept = blue_betas[1],
                colour = "blue") +
    geom_abline(slope = orange_betas[2], intercept = orange_betas[1],
                colour = "orange")
})
```


## Multivariable Model (1/2)
We allow multiple covariates/predictors and the model becomes

$$Y = X \beta$$
Where:

- $Y$ is a vector of length $n \times 1$,
- $X$ is a matrix of size $n \times (p+1)$
- $\beta$ is a vector of length $(p+1)$
- $n$ is the number of observations
- $p$ is the number of predictors

## Multivariable Model (2/2)
We still only focus on one response variable at a time. Having multiple 
response variables is refered to as [multivariate](https://en.wikipedia.org/wiki/Multivariate_analysis)

Regression analysis commonly involves finding an estimate these $\beta$'s

## How Big is $n$ and $p$?
In Genome Wide Association Studies:

- $n \sim 10,000$
- $p \sim 1,000,000$

Google:

- $>2$ trillion Google searches a year 
[source](https://searchengineland.com/google-now-handles-2-999-trillion-searches-per-year-250247)

World Data Centre for Climate:

- $6$ petabytes ($10^{15}$) of data 
[source](https://www.comparebusinessproducts.com/fyi/10-largest-databases-in-the-world)

## Prediction vs Association
In association we care about how individual covariates relate to the response variable.

Example: how does a lake view affect the selling price of a home?


In prediction we do not care about individual covariates, but about how well the overall model 
classifies or predicts. It can be a black box. 

Example: given a home, what do we expect its selling price to be?


## Overfitting


## Underfitting


## Bias - Variance Tradeoff


## High Dimensionality ($p >> n$)
Any covariate can be rewritten as a combination of the others.

## Shrinkage Methods
Shrinkage methods penalize large $\|\beta\|$ to reduce variance,
by turning the least squares optimization problem into a constrained optimization problem.
![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/L1_and_L2_balls.svg/900px-L1_and_L2_balls.svg.png)

## Principal Components
Uses the eigenvalues and eigenvectors of the covariates to rewrite
a set of 